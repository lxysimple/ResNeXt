{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 解析cifar-10数据集\n",
    "运行H_ResNeXt/src/01_parse_cifar10_to_png.py，将cifar-10数据集解析为png格式   \n",
    "\n",
    "数据存放位置：   \n",
    "F:\\cv_paper\\lesson\\Data\\cifar-10\\cifar-10-python.tar   \n",
    "解压得到：  \n",
    "F:\\cv_paper\\lesson\\Data\\cifar-10\\cifar-10-batches-py\n",
    "\n",
    "经过01_parse_cifar10_to_png.py，得到：  \n",
    "F:\\cv_paper\\lesson\\Data\\cifar-10\\cifar10_train   \n",
    "F:\\cv_paper\\lesson\\Data\\cifar-10\\cifar10_test  \n",
    "\n",
    "\n",
    "### 数据展示\n",
    "<img src=\"imgs/cifar10.png\" width=\"700\" heith=\"700\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     57
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "os.environ['NLS_LANG'] = 'SIMPLIFIED CHINESE_CHINA.UTF8'\n",
    "import sys\n",
    "sys.path.append(BASE_DIR)\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from tools.cifar10_dataset import CifarDataset\n",
    "from tools.common_tools import ModelTrainer, show_confMat, plot_line\n",
    "\n",
    "class CifarDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        assert (os.path.exists(data_dir)), \"data_dir:{} 不存在！\".format(data_dir)\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self._get_img_info()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fn, label = self.img_info[index]\n",
    "        img = Image.open(fn).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        if len(self.img_info) == 0:\n",
    "            raise Exception(\"未获取任何图片路径，请检查dataset及文件路径！\")\n",
    "        return len(self.img_info)\n",
    "\n",
    "    def _get_img_info(self):\n",
    "        sub_dir_ = [name for name in os.listdir(self.data_dir) if os.path.isdir(os.path.join(self.data_dir, name))]\n",
    "        sub_dir = [os.path.join(self.data_dir, c) for c in sub_dir_]\n",
    "\n",
    "        self.img_info = []\n",
    "        for c_dir in sub_dir:\n",
    "            path_img = [(os.path.join(c_dir, i), int(os.path.basename(c_dir))) for i in os.listdir(c_dir) if\n",
    "                        i.endswith(\"png\")]\n",
    "            self.img_info.extend(path_img)\n",
    "            \n",
    "            \n",
    "def transform_invert(img_, transform_train):\n",
    "    \"\"\"\n",
    "    将data 进行反transfrom操作\n",
    "    :param img_: tensor\n",
    "    :param transform_train: torchvision.transforms\n",
    "    :return: PIL image\n",
    "    \"\"\"\n",
    "    if 'Normalize' in str(transform_train):\n",
    "        norm_transform = list(filter(lambda x: isinstance(x, transforms.Normalize), transform_train.transforms))\n",
    "        mean = torch.tensor(norm_transform[0].mean, dtype=img_.dtype, device=img_.device)\n",
    "        std = torch.tensor(norm_transform[0].std, dtype=img_.dtype, device=img_.device)\n",
    "        img_.mul_(std[:, None, None]).add_(mean[:, None, None])\n",
    "\n",
    "    img_ = img_.transpose(0, 2).transpose(0, 1)  # C*H*W --> H*W*C\n",
    "    if 'ToTensor' in str(transform_train):\n",
    "        img_ = np.array(img_) * 255\n",
    "\n",
    "    if img_.shape[2] == 3:\n",
    "        img_ = Image.fromarray(img_.astype('uint8')).convert('RGB')\n",
    "    elif img_.shape[2] == 1:\n",
    "        img_ = Image.fromarray(img_.astype('uint8').squeeze())\n",
    "    else:\n",
    "        raise Exception(\"Invalid img shape, expected 1 or 3 in axis 2, but got {}!\".format(img_.shape[2]) )\n",
    "\n",
    "    return img_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     3,
     11
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [5.9216e-01, 6.0000e-01, 6.0000e-01,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [5.9608e-01, 6.0000e-01, 6.0000e-01,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [6.0000e-01, 6.0392e-01, 6.0784e-01,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [6.0392e-01, 6.0784e-01, 6.1176e-01,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[2.9802e-08, 2.9802e-08, 2.9802e-08,  ..., 2.9802e-08,\n",
      "          2.9802e-08, 2.9802e-08],\n",
      "         [2.9802e-08, 2.9802e-08, 2.9802e-08,  ..., 2.9802e-08,\n",
      "          2.9802e-08, 2.9802e-08],\n",
      "         [6.3137e-01, 6.3922e-01, 6.3922e-01,  ..., 2.9802e-08,\n",
      "          2.9802e-08, 2.9802e-08],\n",
      "         ...,\n",
      "         [6.4706e-01, 6.5098e-01, 6.5098e-01,  ..., 2.9802e-08,\n",
      "          2.9802e-08, 2.9802e-08],\n",
      "         [6.5098e-01, 6.5490e-01, 6.5882e-01,  ..., 2.9802e-08,\n",
      "          2.9802e-08, 2.9802e-08],\n",
      "         [6.5490e-01, 6.5882e-01, 6.6275e-01,  ..., 2.9802e-08,\n",
      "          2.9802e-08, 2.9802e-08]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [6.6667e-01, 6.7451e-01, 6.7451e-01,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [6.7843e-01, 6.8235e-01, 6.8235e-01,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [6.8235e-01, 6.8627e-01, 6.9020e-01,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [6.8627e-01, 6.9020e-01, 6.9412e-01,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]]]) 0\n",
      "<PIL.Image.Image image mode=RGB size=32x32 at 0x29CAE078F60>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x29ccd6e3c88>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "# 构建MyDataset实例\n",
    "train_dir = os.path.join(BASE_DIR, \"..\", \"Data\", \"cifar-10\",  \"cifar10_train\")\n",
    "train_data = CifarDataset(data_dir=train_dir, transform=train_transform)\n",
    "\n",
    "print(train_data.__len__()) \n",
    "\n",
    "img_tensor, label = train_data.__getitem__(66)\n",
    "\n",
    "img_rgb = transform_invert(img_tensor, train_transform)\n",
    "print(img_tensor, label)\n",
    "print(img_rgb)\n",
    "plt.imshow(img_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 针对cifar-10的ResNet结构：resnet-20/32/44/56/110/1202\n",
    "\n",
    "参考自：   \n",
    "https://github.com/akamaster/pytorch_resnet_cifar10   \n",
    "https://github.com/TingsongYu/ghostnet_cifar10/tree/master/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''Grouped convolution block.'''\n",
    "    expansion = 2\n",
    "\n",
    "    def __init__(self, in_planes, cardinality=32, bottleneck_width=4, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        group_width = cardinality * bottleneck_width  # 32*4= 128，卷积层有128个卷积核，只不过分成了32组\n",
    "        self.conv1 = nn.Conv2d(in_planes, group_width, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(group_width)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(group_width, group_width, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(group_width)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(group_width, self.expansion*group_width, kernel_size=1, bias=False)  # block最后一个卷积拓展2倍通道数\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*group_width)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*group_width:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*group_width, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*group_width)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))       # 1*1 conv\n",
    "        out = F.relu(self.bn2(self.conv2(out)))     # 3*3 group conv\n",
    "        out = self.bn3(self.conv3(out))             # 1*1 conv\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNeXt(nn.Module):\n",
    "    def __init__(self, num_blocks, cardinality, bottleneck_width, num_classes=10):\n",
    "        super(ResNeXt, self).__init__()\n",
    "        self.cardinality = cardinality            # 32\n",
    "        self.bottleneck_width = bottleneck_width  # 4\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(num_blocks[0], 1)\n",
    "        self.layer2 = self._make_layer(num_blocks[1], 2)\n",
    "        self.layer3 = self._make_layer(num_blocks[2], 2)\n",
    "        self.linear = nn.Linear(cardinality*bottleneck_width*8, num_classes)\n",
    "\n",
    "    def _make_layer(self, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(Block(self.in_planes, self.cardinality, self.bottleneck_width, stride))\n",
    "            self.in_planes = Block.expansion * self.cardinality * self.bottleneck_width\n",
    "        # Increase bottleneck_width by 2 after each stage.\n",
    "        self.bottleneck_width *= 2\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNeXt29_2x64d():\n",
    "    return ResNeXt(num_blocks=[3, 3, 3], cardinality=2, bottleneck_width=64)\n",
    "\n",
    "def ResNeXt29_4x64d():\n",
    "    return ResNeXt(num_blocks=[3, 3, 3], cardinality=4, bottleneck_width=64)\n",
    "\n",
    "def ResNeXt29_8x64d():\n",
    "    return ResNeXt(num_blocks=[3, 3, 3], cardinality=8, bottleneck_width=64)\n",
    "\n",
    "def ResNeXt29_32x4d():\n",
    "    return ResNeXt(num_blocks=[3, 3, 3], cardinality=32, bottleneck_width=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNeXt(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=1024, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ResNeXt29_32x4d_model = ResNeXt29_32x4d() \n",
    "print(ResNeXt29_32x4d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 模型训练器  ModelTrainer\n",
    "\n",
    "定义模型训练类，用于完成模型前向，反向传播，并记录训练loss，accuracy等指标  \n",
    "\n",
    "目的是简化主代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     4,
     43
    ]
   },
   "outputs": [],
   "source": [
    "class ModelTrainer(object):\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def train(data_loader, model, loss_f, optimizer, epoch_id, device, max_epoch):\n",
    "        model.train()\n",
    "\n",
    "        conf_mat = np.zeros((10, 10))   # 混淆矩阵，用于绘图，且计算accuracy，precision，recall等指标很方便\n",
    "        loss_sigma = []\n",
    "\n",
    "        for i, data in enumerate(data_loader):\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_f(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 统计预测信息\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # 统计混淆矩阵\n",
    "            for j in range(len(labels)):\n",
    "                cate_i = labels[j].cpu().numpy()\n",
    "                pre_i = predicted[j].cpu().numpy()\n",
    "                conf_mat[cate_i, pre_i] += 1.\n",
    "\n",
    "            # 统计loss\n",
    "            loss_sigma.append(loss.item())                  # 记录每个iterations的loss，待会取均值就得到epochs的loss\n",
    "            acc_avg = conf_mat.trace() / conf_mat.sum()     # 利用混淆矩阵求取accuracy， 矩阵的迹 除以 总元素 \n",
    "\n",
    "            # 每10个iteration 打印一次训练信息，loss为10个iteration的平均\n",
    "            if i % 50 == 50 - 1:\n",
    "                print(\"Training: Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
    "                    epoch_id + 1, max_epoch, i + 1, len(data_loader), np.mean(loss_sigma), acc_avg))\n",
    "\n",
    "        return np.mean(loss_sigma), acc_avg, conf_mat\n",
    "\n",
    "    @staticmethod\n",
    "    def valid(data_loader, model, loss_f, device):\n",
    "        model.eval()\n",
    "\n",
    "        conf_mat = np.zeros((10, 10))\n",
    "        loss_sigma = []\n",
    "\n",
    "        for i, data in enumerate(data_loader):\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_f(outputs, labels)\n",
    "\n",
    "            # 统计预测信息\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # 统计混淆矩阵\n",
    "            for j in range(len(labels)):\n",
    "                cate_i = labels[j].cpu().numpy()\n",
    "                pre_i = predicted[j].cpu().numpy()\n",
    "                conf_mat[cate_i, pre_i] += 1.\n",
    "\n",
    "            # 统计loss\n",
    "            loss_sigma.append(loss.item())\n",
    "\n",
    "        acc_avg = conf_mat.trace() / conf_mat.sum()\n",
    "\n",
    "        return np.mean(loss_sigma), acc_avg, conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.混淆矩阵概念\n",
    "混淆矩阵(Confusion Matrix)常用来观察分类结果，其是一个N\\*N的方阵，N表示类别数。 \n",
    "\n",
    "混淆矩阵的行表示真实类别，列表示预测类别。例如，猫狗的二分类问题，有猫的图像10张，狗的图像30张，模型对这40张图片进行预测，得到的混淆矩阵为\n",
    "\n",
    "| 类别|  阿猫   | 阿狗  |\n",
    "|----|  ----  | ----  |\n",
    "|阿猫 | 7  | 3 |\n",
    "|阿狗| 10  | 20 |\n",
    "\n",
    "\n",
    "从第一行中可知道，10张猫的图像中，7张预测为猫，3张预测为狗，猫的召回率(Recall)为7/10 = 70%，   \n",
    "从第二行中可知道，30张狗的图像中，8张预测为猫，22张预测为狗，狗的召回率为20/30 = 66.7%，  \n",
    "从第一列中可知道，预测为猫的17张图像中，有7张是真正的猫，猫的精确度(Precision)为7 / 17 = 41.17%   \n",
    "从第二列中可知道，预测为狗的23张图像中，有20张是真正的狗，狗的精确度(Precision)为20 / 23 = 86.96%  \n",
    "\n",
    "模型的准确率(Accuracy)为  (7+20) / 40 = 67.5%   \n",
    "\n",
    "可以发现通过混淆矩阵可以清晰的看出网络模型的分类情况，若再结合上颜色可视化，可方便的看出模型的分类偏好。  \n",
    "\n",
    "\n",
    "<img src=\"imgs/Confusion_Matrixtrain.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_1.4_gpu",
   "language": "python",
   "name": "pytorch_1.4_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
