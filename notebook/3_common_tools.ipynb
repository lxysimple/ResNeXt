{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 解析cifar-10数据集\n",
    "运行H_ResNeXt/src/01_parse_cifar10_to_png.py，将cifar-10数据集解析为png格式   \n",
    "\n",
    "数据存放位置：   \n",
    "F:\\cv_paper\\lesson\\Data\\cifar-10\\cifar-10-python.tar   \n",
    "解压得到：  \n",
    "F:\\cv_paper\\lesson\\Data\\cifar-10\\cifar-10-batches-py\n",
    "\n",
    "经过01_parse_cifar10_to_png.py，得到：  \n",
    "F:\\cv_paper\\lesson\\Data\\cifar-10\\cifar10_train   \n",
    "F:\\cv_paper\\lesson\\Data\\cifar-10\\cifar10_test  \n",
    "\n",
    "\n",
    "### 数据展示\n",
    "<img src=\"imgs/cifar10.png\" width=\"700\" heith=\"700\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     57
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "os.environ['NLS_LANG'] = 'SIMPLIFIED CHINESE_CHINA.UTF8'\n",
    "import sys\n",
    "sys.path.append(BASE_DIR)\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from tools.cifar10_dataset import CifarDataset\n",
    "from tools.common_tools import ModelTrainer, show_confMat, plot_line\n",
    "\n",
    "class CifarDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        assert (os.path.exists(data_dir)), \"data_dir:{} 不存在！\".format(data_dir)\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self._get_img_info()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fn, label = self.img_info[index]\n",
    "        img = Image.open(fn).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        if len(self.img_info) == 0:\n",
    "            raise Exception(\"未获取任何图片路径，请检查dataset及文件路径！\")\n",
    "        return len(self.img_info)\n",
    "\n",
    "    def _get_img_info(self):\n",
    "        sub_dir_ = [name for name in os.listdir(self.data_dir) if os.path.isdir(os.path.join(self.data_dir, name))]\n",
    "        sub_dir = [os.path.join(self.data_dir, c) for c in sub_dir_]\n",
    "\n",
    "        self.img_info = []\n",
    "        for c_dir in sub_dir:\n",
    "            path_img = [(os.path.join(c_dir, i), int(os.path.basename(c_dir))) for i in os.listdir(c_dir) if\n",
    "                        i.endswith(\"png\")]\n",
    "            self.img_info.extend(path_img)\n",
    "            \n",
    "            \n",
    "def transform_invert(img_, transform_train):\n",
    "    \"\"\"\n",
    "    将data 进行反transfrom操作\n",
    "    :param img_: tensor\n",
    "    :param transform_train: torchvision.transforms\n",
    "    :return: PIL image\n",
    "    \"\"\"\n",
    "    if 'Normalize' in str(transform_train):\n",
    "        norm_transform = list(filter(lambda x: isinstance(x, transforms.Normalize), transform_train.transforms))\n",
    "        mean = torch.tensor(norm_transform[0].mean, dtype=img_.dtype, device=img_.device)\n",
    "        std = torch.tensor(norm_transform[0].std, dtype=img_.dtype, device=img_.device)\n",
    "        img_.mul_(std[:, None, None]).add_(mean[:, None, None])\n",
    "\n",
    "    img_ = img_.transpose(0, 2).transpose(0, 1)  # C*H*W --> H*W*C\n",
    "    if 'ToTensor' in str(transform_train):\n",
    "        img_ = np.array(img_) * 255\n",
    "\n",
    "    if img_.shape[2] == 3:\n",
    "        img_ = Image.fromarray(img_.astype('uint8')).convert('RGB')\n",
    "    elif img_.shape[2] == 1:\n",
    "        img_ = Image.fromarray(img_.astype('uint8').squeeze())\n",
    "    else:\n",
    "        raise Exception(\"Invalid img shape, expected 1 or 3 in axis 2, but got {}!\".format(img_.shape[2]) )\n",
    "\n",
    "    return img_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     3,
     11
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.7255e-01,\n",
      "          5.1373e-01, 4.5882e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.9412e-01,\n",
      "          4.3922e-01, 4.0000e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.7059e-01,\n",
      "          5.1765e-01, 4.4314e-01],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[2.9802e-08, 2.9802e-08, 2.9802e-08,  ..., 6.4314e-01,\n",
      "          5.8431e-01, 5.2549e-01],\n",
      "         [2.9802e-08, 2.9802e-08, 2.9802e-08,  ..., 5.4902e-01,\n",
      "          4.7843e-01, 4.5882e-01],\n",
      "         [2.9802e-08, 2.9802e-08, 2.9802e-08,  ..., 4.9804e-01,\n",
      "          5.0980e-01, 4.8235e-01],\n",
      "         ...,\n",
      "         [2.9802e-08, 2.9802e-08, 2.9802e-08,  ..., 2.9802e-08,\n",
      "          2.9802e-08, 2.9802e-08],\n",
      "         [2.9802e-08, 2.9802e-08, 2.9802e-08,  ..., 2.9802e-08,\n",
      "          2.9802e-08, 2.9802e-08],\n",
      "         [2.9802e-08, 2.9802e-08, 2.9802e-08,  ..., 2.9802e-08,\n",
      "          2.9802e-08, 2.9802e-08]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.7059e-01,\n",
      "          6.1176e-01, 5.5294e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.8039e-01,\n",
      "          5.0980e-01, 4.8235e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.4118e-01,\n",
      "          5.5294e-01, 4.9804e-01],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]]]) 0\n",
      "<PIL.Image.Image image mode=RGB size=32x32 at 0x1D351BE3390>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d36edf66d8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "# 构建MyDataset实例\n",
    "train_dir = os.path.join(BASE_DIR, \"..\", \"Data\", \"cifar-10\",  \"cifar10_train\")\n",
    "train_data = CifarDataset(data_dir=train_dir, transform=train_transform)\n",
    "\n",
    "print(train_data.__len__()) \n",
    "\n",
    "img_tensor, label = train_data.__getitem__(66)\n",
    "\n",
    "img_rgb = transform_invert(img_tensor, train_transform)\n",
    "print(img_tensor, label)\n",
    "print(img_rgb)\n",
    "plt.imshow(img_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 针对cifar-10的ResNeXt结构\n",
    "参考：https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnext.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''Grouped convolution block.'''\n",
    "    expansion = 2\n",
    "\n",
    "    def __init__(self, in_planes, cardinality=32, bottleneck_width=4, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        group_width = cardinality * bottleneck_width  # 32*4= 128，卷积层有128个卷积核，只不过分成了32组\n",
    "        self.conv1 = nn.Conv2d(in_planes, group_width, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(group_width)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(group_width, group_width, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(group_width)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(group_width, self.expansion*group_width, kernel_size=1, bias=False)  # block最后一个卷积拓展2倍通道数\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*group_width)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*group_width:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*group_width, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*group_width)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))       # 1*1 conv\n",
    "        out = F.relu(self.bn2(self.conv2(out)))     # 3*3 group conv\n",
    "        out = self.bn3(self.conv3(out))             # 1*1 conv\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNeXt(nn.Module):\n",
    "    def __init__(self, num_blocks, cardinality, bottleneck_width, num_classes=10):\n",
    "        super(ResNeXt, self).__init__()\n",
    "        self.cardinality = cardinality            # 32\n",
    "        self.bottleneck_width = bottleneck_width  # 4\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(num_blocks[0], 1)\n",
    "        self.layer2 = self._make_layer(num_blocks[1], 2)\n",
    "        self.layer3 = self._make_layer(num_blocks[2], 2)\n",
    "        self.linear = nn.Linear(cardinality*bottleneck_width*8, num_classes)\n",
    "\n",
    "    def _make_layer(self, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(Block(self.in_planes, self.cardinality, self.bottleneck_width, stride))\n",
    "            self.in_planes = Block.expansion * self.cardinality * self.bottleneck_width\n",
    "        # Increase bottleneck_width by 2 after each stage.\n",
    "        self.bottleneck_width *= 2\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNeXt29_2x64d():\n",
    "    return ResNeXt(num_blocks=[3, 3, 3], cardinality=2, bottleneck_width=64)\n",
    "\n",
    "def ResNeXt29_4x64d():\n",
    "    return ResNeXt(num_blocks=[3, 3, 3], cardinality=4, bottleneck_width=64)\n",
    "\n",
    "def ResNeXt29_8x64d():\n",
    "    return ResNeXt(num_blocks=[3, 3, 3], cardinality=8, bottleneck_width=64)\n",
    "\n",
    "def ResNeXt29_32x4d():\n",
    "    return ResNeXt(num_blocks=[3, 3, 3], cardinality=32, bottleneck_width=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNeXt(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Block(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=1024, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ResNeXt29_32x4d_model = ResNeXt29_32x4d() \n",
    "print(ResNeXt29_32x4d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 模型训练器  ModelTrainer\n",
    "\n",
    "定义模型训练类，用于完成模型前向，反向传播，并记录训练loss，accuracy等指标  \n",
    "\n",
    "目的是简化主代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     4,
     43
    ]
   },
   "outputs": [],
   "source": [
    "class ModelTrainer(object):\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def train(data_loader, model, loss_f, optimizer, epoch_id, device, max_epoch):\n",
    "        model.train()\n",
    "\n",
    "        conf_mat = np.zeros((10, 10))   # 混淆矩阵，用于绘图，且计算accuracy，precision，recall等指标很方便\n",
    "        loss_sigma = []\n",
    "\n",
    "        for i, data in enumerate(data_loader):\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_f(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 统计预测信息\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # 统计混淆矩阵\n",
    "            for j in range(len(labels)):\n",
    "                cate_i = labels[j].cpu().numpy()\n",
    "                pre_i = predicted[j].cpu().numpy()\n",
    "                conf_mat[cate_i, pre_i] += 1.\n",
    "\n",
    "            # 统计loss\n",
    "            loss_sigma.append(loss.item())                  # 记录每个iterations的loss，待会取均值就得到epochs的loss\n",
    "            acc_avg = conf_mat.trace() / conf_mat.sum()     # 利用混淆矩阵求取accuracy， 矩阵的迹 除以 总元素 \n",
    "\n",
    "            # 每10个iteration 打印一次训练信息，loss为10个iteration的平均\n",
    "            if i % 50 == 50 - 1:\n",
    "                print(\"Training: Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
    "                    epoch_id + 1, max_epoch, i + 1, len(data_loader), np.mean(loss_sigma), acc_avg))\n",
    "\n",
    "        return np.mean(loss_sigma), acc_avg, conf_mat\n",
    "\n",
    "    @staticmethod\n",
    "    def valid(data_loader, model, loss_f, device):\n",
    "        model.eval()\n",
    "\n",
    "        conf_mat = np.zeros((10, 10))\n",
    "        loss_sigma = []\n",
    "\n",
    "        for i, data in enumerate(data_loader):\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_f(outputs, labels)\n",
    "\n",
    "            # 统计预测信息\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # 统计混淆矩阵\n",
    "            for j in range(len(labels)):\n",
    "                cate_i = labels[j].cpu().numpy()\n",
    "                pre_i = predicted[j].cpu().numpy()\n",
    "                conf_mat[cate_i, pre_i] += 1.\n",
    "\n",
    "            # 统计loss\n",
    "            loss_sigma.append(loss.item())\n",
    "\n",
    "        acc_avg = conf_mat.trace() / conf_mat.sum()\n",
    "\n",
    "        return np.mean(loss_sigma), acc_avg, conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.混淆矩阵概念\n",
    "混淆矩阵(Confusion Matrix)常用来观察分类结果，其是一个N\\*N的方阵，N表示类别数。 \n",
    "\n",
    "混淆矩阵的行表示真实类别，列表示预测类别。例如，猫狗的二分类问题，有猫的图像10张，狗的图像30张，模型对这40张图片进行预测，得到的混淆矩阵为\n",
    "\n",
    "| 类别|  阿猫   | 阿狗  |\n",
    "|----|  ----  | ----  |\n",
    "|阿猫 | 7  | 3 |\n",
    "|阿狗| 10  | 20 |\n",
    "\n",
    "\n",
    "从第一行中可知道，10张猫的图像中，7张预测为猫，3张预测为狗，猫的召回率(Recall)为7/10 = 70%，   \n",
    "从第二行中可知道，30张狗的图像中，8张预测为猫，22张预测为狗，狗的召回率为20/30 = 66.7%，  \n",
    "从第一列中可知道，预测为猫的17张图像中，有7张是真正的猫，猫的精确度(Precision)为7 / 17 = 41.17%   \n",
    "从第二列中可知道，预测为狗的23张图像中，有20张是真正的狗，狗的精确度(Precision)为20 / 23 = 86.96%  \n",
    "\n",
    "模型的准确率(Accuracy)为  (7+20) / 40 = 67.5%   \n",
    "\n",
    "可以发现通过混淆矩阵可以清晰的看出网络模型的分类情况，若再结合上颜色可视化，可方便的看出模型的分类偏好。  \n",
    "\n",
    "\n",
    "<img src=\"imgs/Confusion_Matrixtrain.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VGG",
   "language": "python",
   "name": "vgg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
